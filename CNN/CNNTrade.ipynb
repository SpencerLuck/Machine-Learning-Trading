{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe3a660",
   "metadata": {},
   "source": [
    "# CNN Trading Prediction\n",
    "\n",
    "This notebook seeks to create a convolutional neural network that makes an expectation as to the sign of the next price return, in other words whether price is going to increase or decrease in the next time period.\n",
    "\n",
    "The motivation for using a CNN is it's common application in feature extraction from image data, the hope would be that a CNN could extract abstract features from time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing technical indicator library for the Average True Range\n",
    "!pip install pandas_ta\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 20\n",
    "\n",
    "# Predicting one time period into the future \n",
    "PREDICT_HORIZON = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(current_close, future_close):\n",
    "    if float(future_close) > float(current_close):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "\n",
    "    for col in df.columns: \n",
    "        col_end = col.rsplit('_', -1)[-1] # get the last word of the string\n",
    "        if col != 'target':\n",
    "            if col_end == 'volume':\n",
    "                df[col] = (df[col].values - df[col].mean())/df[col].std() # standardizing volume (mean 0 , std 1)\n",
    "\n",
    "            elif col_end == 'value':\n",
    "                df[col] = (df[col].values - df[col].mean())/df[col].std() # standardizing indicator values (mean 0 , std 1)\n",
    "\n",
    "            elif col_end == 'open' or col_end == 'high' or col_end == 'low' or col_end == 'close': \n",
    "                df[col] = (df[col].values - df[col].mean())/df[col].std() # standardizing price returns\n",
    "\n",
    "    df.dropna(inplace=True)  \n",
    "\n",
    "\n",
    "    sequential_data = []  \n",
    "    prev_days = deque(maxlen=SEQ_LEN)  \n",
    "\n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store each row excluding target\n",
    "        if len(prev_days) == SEQ_LEN:  # 20 sequences \n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  \n",
    "\n",
    "    random.shuffle(sequential_data)  \n",
    "\n",
    "    buys = []  \n",
    "    sells = []  \n",
    "    flats = []\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    for seq, target in sequential_data:  \n",
    "        if target == 0:  # sell\n",
    "            sells.append([seq, target])  \n",
    "        if target == 1:  # flat\n",
    "            flats.append([seq, target])  \n",
    "        elif target == 2:  # buy\n",
    "            buys.append([seq, target])  \n",
    "\n",
    "    \n",
    "    random.shuffle(buys)  \n",
    "    random.shuffle(sells)  \n",
    "    random.shuffle(flats)  \n",
    "    \n",
    "    # Balancing our dataset\n",
    "    lower = min(len(buys), len(sells), len(flats))  \n",
    "\n",
    "    buys = buys[:lower]  \n",
    "    flats = flats[:lower]\n",
    "    sells = sells[:lower] \n",
    "\n",
    "    sequential_data = buys+flats+sells  \n",
    "    random.shuffle(sequential_data)  \n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data: \n",
    "        X.append(seq)  \n",
    "        y.append(target)  \n",
    "\n",
    "    return np.array(X), y  \n",
    "\n",
    "\n",
    "\n",
    "def preprocess_stats_df(main_df):\n",
    "    \n",
    "    df = main_df.copy()\n",
    "    stats_dict = {}\n",
    "                      \n",
    "    for col in df.columns:\n",
    "        col_end = col.rsplit('_', -1)[-1] # get the last word of the string\n",
    "        if col_end == 'volume':\n",
    "            mean = df[col].mean()\n",
    "            std = df[col].std()\n",
    "            stats_dict[f'{col}'] = {'mean': mean, 'std': std}\n",
    "\n",
    "        elif col_end == 'value':\n",
    "            mean = df[col].mean()\n",
    "            std = df[col].std()\n",
    "            stats_dict[f'{col}'] = {'mean': mean, 'std': std}\n",
    "\n",
    "        elif col_end == 'open' or col_end == 'high' or col_end == 'low' or col_end == 'close':\n",
    "            mean = df[col].mean()\n",
    "            std = df[col].std()\n",
    "            stats_dict[f'{col}'] = {'mean': mean, 'std': std}\n",
    "\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ad39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crypto hourly data\n",
    "path = ''\n",
    "main_df = pd.read_csv(path)\n",
    "main_df.set_index('time', inplace=True)\n",
    "\n",
    "main_df.sort_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd30c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time transformations\n",
    "\n",
    "day = 60*60*24 # seconds in the day\n",
    "week = 60*60*24*7 # seconds in week\n",
    "\n",
    "main_df['Day_sin'] = np.sin(main_df.index * (2 * np.pi / day))\n",
    "main_df['Day_cos'] = np.cos(main_df.index * (2 * np.pi / day))\n",
    "main_df['Week_sin'] = np.sin(main_df.index * (2 * np.pi / week))\n",
    "main_df['Week_cos'] = np.cos(main_df.index * (2 * np.pi / week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f222b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "\n",
    "main_df['willr_value'] = ta.willr(close=main_df4['close'], high=main_df['high'], low=main_df4['low'])\n",
    "main_df['nvi_value'] = ta.nvi(close=main_df['close'], volume=main_df['volume'])\n",
    "\n",
    "for i in [1, 2, 3, 5, 10, 20]:    \n",
    "    main_df[f'ret_{i}_close'] = main_df['close'].pct_change(i)\n",
    "    main_df[f'ret_{i}_high'] = main_df['high'].pct_change(i)\n",
    "    main_df[f'ret_{i}_low'] = main_df['low'].pct_change(i)\n",
    "    main_df[f'ret_{i}_open'] = main_df['open'].pct_change(i)\n",
    "    main_df[f'ret_{i}_volume'] = main_df['volume'].pct_change(i)\n",
    "\n",
    "for i in [5, 10, 20, 40]:\n",
    "    main_df[f'volt_{i}_close'] = np.log(1 + main_df['ret_1_close']).rolling(i).std()\n",
    "    \n",
    "\n",
    "main_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "main_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ca844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats dictionary for deployment (best practice to keep commented when not needing it)\n",
    "# stats = preprocess_stats_df(main_df)\n",
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccf21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column for future close\n",
    "main_df['future_close_1'] = main_df['close'].shift(-PREDICT_HORIZON1)\n",
    "\n",
    "main_df.dropna(inplace=True)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating target column. Mapping classify function \n",
    "main_df['target'] = list(map(classify, main_df['close'], main_df['future_close_1']))\n",
    "\n",
    "main_df = main_df.drop('future_close_1', axis=1)\n",
    "\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b16db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of sorted times to split into train and validation sets\n",
    "times = sorted(main_df.index.values)\n",
    "\n",
    "main_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "main_df.dropna(inplace=True)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the last 20% threshold\n",
    "last_20pct = times[-int(0.20*len(times))]\n",
    "\n",
    "# Splitting into train and validation\n",
    "validation_main_df = main_df[(main_df.index >= last_20pct)]\n",
    "main_df = main_df[(main_df.index < last_20pct)]\n",
    "\n",
    "# Preprocessing, X and y\n",
    "train_x, train_y = preprocess(main_df)\n",
    "validation_x, validation_y = preprocess(validation_main_df)\n",
    "\n",
    "# Checking the outcome of the above\n",
    "print(f'train data: {len(train_x)} validation: {len(validation_x)}')\n",
    "print(f'positive returns: {train_y.count(1)}, negative returns: {train_y.count(0)})\n",
    "print(f'VALIDATION positive returns: {validation_y.count(1)}, negative returns: {validation_y.count(0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating arrays for our neural network\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)\n",
    "validation_x = np.asarray(validation_x)\n",
    "validation_y = np.asarray(validation_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c077adc",
   "metadata": {},
   "source": [
    "### Pure CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17141043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(15, kernel_size=3, activation = 'tanh', input_shape=(train_x.shape[1:])))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6, amsgrad=True)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "filepath = 'NETWORK_NAME_{epoch:02d}'\n",
    "checkpoint = ModelCheckpoint('save_path/{}.model'.format(save_path, filepath, monitor='val_loss', \n",
    "                                                  verbose=1, save_best_only=True, mode='min')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ab972",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[checkpoint]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ba53d",
   "metadata": {},
   "source": [
    "### Alternate CNN LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f7872",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(TimeDistributed(Conv1D(filters=20,\n",
    "                                  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                                  bias_regularizer=regularizers.L2(1e-5),\n",
    "                                  activity_regularizer=regularizers.L2(1e-5),\n",
    "                                  kernel_size=4, \n",
    "                                  activation='tanh'), input_shape=(None, 20, 9)))\n",
    "model2.add(TimeDistributed(MaxPooling1D(pool_size=2, strides=None)))\n",
    "model2.add(TimeDistributed(Flatten()))\n",
    "model2.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model2.add(Bidirectional(LSTM(32, \n",
    "                              kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                              bias_regularizer=regularizers.L2(1e-5),\n",
    "                              activity_regularizer=regularizers.L2(1e-5), \n",
    "                              activation='tanh', input_shape=(train_x.shape[1:]))))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.1))\n",
    "\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.1))\n",
    "\n",
    "model2.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6, amsgrad=True)\n",
    "\n",
    "# Compile model\n",
    "model2.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "filepath = 'NETWORK_2_NAME_{epoch:02d}'\n",
    "checkpoint = ModelCheckpoint('save_path/{}.model'.format(save_path, filepath, monitor='val_loss', \n",
    "                                                  verbose=1, save_best_only=True, mode='min')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a484a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[checkpoint]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
